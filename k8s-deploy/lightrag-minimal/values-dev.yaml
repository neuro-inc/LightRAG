# Development/Minikube Values
# Optimized for local development with reduced resource requirements

# Environment configuration
env:
  LLM_MODEL: "gpt-4o"
  WEBUI_TITLE: "Apolo Copilot - LightRAG (Development)"
  WEBUI_DESCRIPTION: "Development LightRAG for Apolo Documentation"
  
  # OpenAI API Configuration
  LLM_BINDING: "openai"
  LLM_BINDING_HOST: "https://api.openai.com/v1"
  EMBEDDING_BINDING: "openai"
  EMBEDDING_BINDING_HOST: "https://api.openai.com/v1"
  EMBEDDING_MODEL: "text-embedding-ada-002"
  EMBEDDING_DIM: "1536"
  
  # Concurrency settings (conservative for API stability)
  MAX_ASYNC: "4"
  MAX_PARALLEL_INSERT: "2"
  
  # LLM Configuration
  ENABLE_LLM_CACHE: "true"
  ENABLE_LLM_CACHE_FOR_EXTRACT: "true"
  TIMEOUT: "240"
  TEMPERATURE: "0"
  MAX_TOKENS: "32768"

# Reduced resources for local development
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 250m
    memory: 512Mi

# Smaller storage for development
persistence:
  ragStorage:
    size: 5Gi
  inputs:
    size: 2Gi

# PostgreSQL with reduced resources
postgresql:
  # Use pgvector image for vector support
  image:
    registry: docker.io
    repository: pgvector/pgvector
    tag: pg16
  auth:
    password: "dev-lightrag-pass"
  primary:
    persistence:
      size: 5Gi
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 256Mi

# OpenAI API key (set via environment variable)
secrets:
  openaiApiKey: "${OPENAI_API_KEY}"

# Disable ingress for local development (use port-forward)
ingress:
  enabled: false

# Disable autoscaling for development
autoscaling:
  enabled: false